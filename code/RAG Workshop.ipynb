{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG) with NVIDIA NIM Endpoints\n",
    "\n",
    "This notebook demonstrates how to implement a RAG workflow using NVIDIA-hosted model endpoints from build.nvidia.com, with the option to use local NVIDIA GPUs for speeding up a local vector store, if available:\n",
    "- nv-embedqa-e5-v5 for embeddings\n",
    "- nv-rerankqa-mistral-4b-v3 for reranking\n",
    "- llama3.3-70B for text generation\n",
    "- FAISS as a local vector store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull NVIDIA SDKs\n",
    "Fetches the latest information on NVIDIA SDKs from our NVIDIA Developer website. This information is ultimately used to inform our language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%run -i scrapeSDKs.py #Takes about 1.5 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "This code cell imports all of the necessary packages to run the rest of the notebook. Prominent packages are:\n",
    "1. Langchain: To handle all of the document preparation and LLM interaction\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings, NVIDIARerank\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_nvidia import register_model, Model\n",
    "from IPython.display import display, Markdown\n",
    "import ipywidgets as widgets\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool, WheelZoomTool\n",
    "from bokeh.palettes import Category10\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import normalize\n",
    "from typing import List, Tuple\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize API Key\n",
    "This code cell pulls the NVIDIA_API_Key that was stored as a *Secret* within AI Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid NVIDIA_API_KEY already in environment. Delete to reset\n"
     ]
    }
   ],
   "source": [
    "nvAPI_Name = \"NVIDIA_API_KEY\"\n",
    "\n",
    "if os.environ.get(nvAPI_Name, \"\").startswith(\"nvapi-\"):\n",
    "    print(f\"Valid {nvAPI_Name} already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = os.environ.get(nvAPI_Name)\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\\nHere are your listed environment variables: {print(os.environ)}\"\n",
    "    os.environ[nvAPI_Name] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not sure if needed for normal endpoint\n",
    "register_model(Model(\n",
    "        id=\"nvdev/nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "        model_type=\"ranking\",\n",
    "        client=\"NVIDIARerank\",\n",
    "        endpoint=\"https://ai.api.nvidia.com/v1/nvdev/retrieval/nvidia/nv-rerankqa-mistral-4b-v3/reranking\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for GPU Availability\n",
    "\n",
    "If you do not see a GPU available and your host machine has one, **make sure you allocated 1 or more GPUs** in the bottom section called *\"Hardware\"* of the *\"Environment\"* tab within the AI Workbench UI. If you need to make this correction, you will need to rebuild your Environment again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "NVIDIA RTX 3500 Ada Generation Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi --query-gpu=gpu_name --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Declarations\n",
    "Eventually move out to a separate file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to load in .txt, .pdf, .doc, and .docx files, only handles text, not tables or images.\n",
    "def load_documents(directory, docs=[]):\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if filename.endswith('.txt'):\n",
    "            loader = TextLoader(filepath)\n",
    "            docs.extend(loader.load())\n",
    "        elif filename.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(filepath)\n",
    "            docs.extend(loader.load())\n",
    "        elif filename.endswith(('.doc', '.docx')):\n",
    "            loader = Docx2txtLoader(filepath)\n",
    "            docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTags(docs):\n",
    "    for doc in docs:\n",
    "        metadata = doc.metadata\n",
    "        doc_label = metadata.get(\"source\")\n",
    "        if 'partner-docs' in doc_label:\n",
    "            doc.metadata['tag'] = 'Partner'\n",
    "        elif 'rfi-docs' in doc_label:\n",
    "            doc.metadata['tag'] = 'RFI'\n",
    "        else: \n",
    "            doc.metadata['tag'] = 'NVIDIA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import FileUpload, IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "def save_uploaded_files(change, target_dir):\n",
    "    if file_upload.value:\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(target_dir):\n",
    "            os.makedirs(target_dir)\n",
    "            \n",
    "        # Create and display progress bar\n",
    "        progress = IntProgress(\n",
    "            min=0,\n",
    "            max=len(file_upload.value),\n",
    "            description='Uploading:',\n",
    "            bar_style='info'\n",
    "        )\n",
    "        display(progress)\n",
    "\n",
    "        # Iterate over each uploaded file\n",
    "        for i, file in enumerate(file_upload.value):\n",
    "            content = file['content']\n",
    "            filename = file['name']\n",
    "            filepath = os.path.join(target_dir, filename)\n",
    "            \n",
    "            # Save the file\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(content)\n",
    "            \n",
    "            print(f\"Saved {filename} to {filepath}\")\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress.value = i + 1\n",
    "        \n",
    "        # Clear the file upload widget after saving files\n",
    "        file_upload.value = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callback(target_dir):\n",
    "    def callback(change):\n",
    "        save_uploaded_files(change, target_dir)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(query: str, documents: list, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Rerank documents using BGE-reranker-base cross-encoder.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query\n",
    "        documents (list): List of documents to rerank\n",
    "        top_k (int): Number of top documents to return\n",
    "        \n",
    "    Returns:\n",
    "        list: Top k documents with their scores\n",
    "    \"\"\"\n",
    "    # Initialize the cross-encoder\n",
    "    model = CrossEncoder('BAAI/bge-reranker-base', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Create pairs and get scores\n",
    "    pairs = [[query, doc.page_content] for doc in documents]\n",
    "    scores = model.predict(pairs)\n",
    "    \n",
    "    # Combine documents with scores and sort\n",
    "    doc_scores = list(zip(documents, scores))\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return just the documents in ranked order\n",
    "    return [doc for doc, _ in doc_scores[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG function with vector store option\n",
    "def rag_query(query, model, vectorstore=None, rerank=False, show_docs=False, top_k=10, tag='tag', tag_values=['NVIDIA', 'Partner']):\n",
    "    if vectorstore:\n",
    "        # Retrieve relevant documents\n",
    "        retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": top_k})\n",
    "        docs = retriever.invoke(query, filter={tag: {\"$in\": tag_values}})\n",
    "        \n",
    "        # potential to include Rerank\n",
    "        if rerank:\n",
    "            print(f'Docs: {docs}')\n",
    "            reranked_docs = rerank_results(query, docs, top_k=top_k)\n",
    "            \n",
    "            print(f'Rereanked: {reranked_docs}')\n",
    "        else:\n",
    "            reranked_docs = docs[:top_k]\n",
    "        context = \"\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "    else:\n",
    "        context = \"\"\n",
    "    \n",
    "    # Generate response using the LLM\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    if show_docs:\n",
    "        return response, reranked_docs\n",
    "    \n",
    "    try:\n",
    "        ans = response.content\n",
    "        return ans\n",
    "    except AttributeError:\n",
    "        return response\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M2BERTEmbeddings(Embeddings):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"togethercomputer/m2-bert-80M-2k-retrieval\",\n",
    "        max_seq_length: int = 2048,\n",
    "        device: str = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ):\n",
    "        self.device = device\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize model with output_hidden_states=True\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True,\n",
    "            output_hidden_states=True  # Enable hidden states output\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\",\n",
    "            model_max_length=max_seq_length\n",
    "        )\n",
    "        \n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.model.eval()\n",
    "    \n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_seq_length,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Get the last hidden state\n",
    "            embeddings = outputs['sentence_embedding']\n",
    "        \n",
    "        return embeddings[0].tolist()\n",
    "    \n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
    "        return [self._get_embedding(text) for text in texts]\n",
    "    \n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for a query.\"\"\"\n",
    "        return self._get_embedding(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bokeh_plot(data, annotations, classes, colors, title):\n",
    "    data = {\n",
    "    'x': data[:, 0],\n",
    "    'y': data[:, 1],\n",
    "    'filename': annotations,\n",
    "    'label': classes,\n",
    "    'color': colors  # Add color information to the data source\n",
    "    }\n",
    "    source = ColumnDataSource(data)\n",
    "    \n",
    "    # Create a Bokeh figure\n",
    "    p = figure(\n",
    "        title=title,\n",
    "        tools=\"pan,wheel_zoom,reset\",\n",
    "        width=800,\n",
    "        height=600,\n",
    "        tooltips=[(\"Filename\", \"@filename\"), (\"X\", \"@x\"), (\"Y\", \"@y\")]\n",
    "    )\n",
    "    \n",
    "    # Add scatter points with color and legend grouping by 'label'\n",
    "    p.scatter('x', 'y', size=8, source=source, fill_alpha=0.6, color='color', legend_field='label')\n",
    "    \n",
    "    # Customize the legend\n",
    "    p.legend.title = \"Labels\"\n",
    "    p.legend.location = \"top_right\"\n",
    "    p.legend.click_policy = \"hide\"  # Allow interactive toggling of labels\n",
    "\n",
    "    wheel_zoom_tool = p.select_one(WheelZoomTool)\n",
    "    p.toolbar.active_scroll = wheel_zoom_tool\n",
    "    \n",
    "    hover_tool = p.select_one(HoverTool)\n",
    "    p.toolbar.active_inspect = [hover_tool]\n",
    "    \n",
    "    # Show the plot\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Upload and Processing\n",
    "\n",
    "Now, we'll upload a document and process it for our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvDIR = \"../data/nvidia-text-archive\"\n",
    "nvSummaries = \"../data/nvidia-text-archive/processed\"\n",
    "partnerDIR = \"../data/partner-docs\"\n",
    "rfiDIR = \"../data/rfi-docs\"\n",
    "for dir in [nvDIR, nvSummaries, partnerDIR]:\n",
    "    if not os.path.exists(dir):\n",
    "        print(f'Creating Directory: {dir}')\n",
    "        os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c8714cc3874069afcbb78e569c209b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.txt, .pdf, .doc, .docx', description='Partner File Upload', layout=Layout(width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload functionality for partner documents\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.txt, .pdf, .doc, .docx',\n",
    "    multiple=True,\n",
    "    description='Partner File Upload',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "callback = create_callback(partnerDIR)\n",
    "file_upload.observe(callback, names='value')\n",
    "display(file_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d750d63e7d3f4186a6b0389427c9c96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), description='RFI Upload', layout=Layout(width='500px'), multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Upload functionality for RFI\n",
    "file_upload = widgets.FileUpload(\n",
    "    allow='.txt, .pdf, .doc, .docx',\n",
    "    multiple=True,\n",
    "    description='RFI Upload',\n",
    "    layout=widgets.Layout(width='500px')\n",
    ")\n",
    "\n",
    "callback = create_callback(rfiDIR)\n",
    "file_upload.observe(callback, names='value')\n",
    "display(file_upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = load_documents(nvDIR, []) #Load in NVIDIA documents\n",
    "documents = load_documents(partnerDIR, documents) #Append partner documents\n",
    "documents = load_documents(rfiDIR, documents) #Append RFI\n",
    "\n",
    "chunk_size = 800\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=40\n",
    ")\n",
    "\n",
    "chunked_documents = text_splitter.split_documents(documents)\n",
    "addTags(chunked_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA: 12.4\n",
      "GPU Available: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"PyTorch CUDA: {torch.version.cuda}\")  # Should show 12.4\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")  # Should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type m2_bert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "-- Bidirectional: True\n",
      "-- Using Long Conv Residual: True\n",
      "-- Hyena w: 10\n",
      "-- Hyena w mod: 1\n",
      "-- Hyena filter order: 128\n",
      "-- Hyena filter dropout: 0.2\n",
      "-- Hyena filter wd: 0.1\n",
      "-- Hyena filter emb dim: 5\n",
      "-- Hyena filter lr: 0.001\n",
      "-- Hyena filter lr pos emb: 1e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m M2BERTEmbeddings(max_seq_length\u001b[38;5;241m=\u001b[39mchunk_size)\n\u001b[0;32m----> 2\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunked_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#this will take ~20 seconds if you have a GPU\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_core/vectorstores/base.py:843\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[1;32m    841\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[0;32m--> 843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:1043\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \n\u001b[1;32m   1027\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[1;32m   1045\u001b[0m         texts,\n\u001b[1;32m   1046\u001b[0m         embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1051\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[19], line 52\u001b[0m, in \u001b[0;36mM2BERTEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate embeddings for a list of documents.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_embedding(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "Cell \u001b[0;32mIn[19], line 52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[List[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m     51\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate embeddings for a list of documents.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "Cell \u001b[0;32mIn[19], line 44\u001b[0m, in \u001b[0;36mM2BERTEmbeddings._get_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Generate embeddings\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 44\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Get the last hidden state\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/togethercomputer/m2-bert-80M-2k-retrieval/0d5e23e332fc54a2dc445636fd4f0821c78fbeb1/bert_layers.py:956\u001b[0m, in \u001b[0;36mBertForTextEncoding.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    952\u001b[0m     masked_tokens_mask \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    954\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 956\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmasked_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasked_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m: pooled_output}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/togethercomputer/m2-bert-80M-2k-retrieval/0d5e23e332fc54a2dc445636fd4f0821c78fbeb1/bert_layers.py:528\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, position_ids, output_all_encoded_layers, masked_tokens_mask, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m     first_col_mask[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    526\u001b[0m     subset_mask \u001b[38;5;241m=\u001b[39m masked_tokens_mask \u001b[38;5;241m|\u001b[39m first_col_mask\n\u001b[0;32m--> 528\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_encoded_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_encodings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_encodings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m masked_tokens_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/togethercomputer/m2-bert-80M-2k-retrieval/0d5e23e332fc54a2dc445636fd4f0821c78fbeb1/bert_layers.py:371\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers, subset_mask, position_encodings)\u001b[0m\n\u001b[1;32m    369\u001b[0m all_encoder_layers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer:\n\u001b[0;32m--> 371\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi_attn_mask\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m position_encodings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m position_encodings\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/togethercomputer/m2-bert-80M-2k-retrieval/0d5e23e332fc54a2dc445636fd4f0821c78fbeb1/bert_layers.py:280\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, cu_seqlens, seqlen, subset_idx, indices, attn_mask, bias)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m     bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    267\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass for a BERT layer, including both attention and MLP.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m        bias: None or (batch, heads, max_seqlen_in_batch, max_seqlen_in_batch)\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(attention_output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m    282\u001b[0m         attention_output, _ \u001b[38;5;241m=\u001b[39m attention_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/togethercomputer/m2-bert-80M-2k-retrieval/0d5e23e332fc54a2dc445636fd4f0821c78fbeb1/monarch_mixer_sequence_mixer.py:108\u001b[0m, in \u001b[0;36mMonarchMixerSequenceMixing.forward\u001b[0;34m(self, u, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# in projection\u001b[39;00m\n\u001b[1;32m    107\u001b[0m u_orig \u001b[38;5;241m=\u001b[39m u\n\u001b[0;32m--> 108\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m u \u001b[38;5;241m=\u001b[39m rearrange(u, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb l d -> b d l\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# short filter\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embeddings = M2BERTEmbeddings(max_seq_length=chunk_size)\n",
    "vectorstore = FAISS.from_documents(chunked_documents, embeddings) #this will take ~20 seconds if you have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = vectorstore.index\n",
    "docstore = vectorstore.docstore\n",
    "\n",
    "num_docs = len(docstore._dict)\n",
    "embeddings = []\n",
    "labels = []\n",
    "filenames = []\n",
    "for i in range(num_docs):\n",
    "    emb = index.reconstruct(i)\n",
    "    embeddings.append(emb)\n",
    "    doc_id = vectorstore.index_to_docstore_id[i]\n",
    "    metadata = docstore.search(str(doc_id)).metadata\n",
    "    doc_label = metadata.get(\"source\", f\"doc_{i}\")\n",
    "    \n",
    "    if 'partner-docs' in doc_label:\n",
    "        filenames.append(doc_label)\n",
    "        label = 'Partner'\n",
    "    elif 'rfi-docs' in doc_label:\n",
    "        filenames.append(doc_label)\n",
    "        label = 'RFI'\n",
    "    else: \n",
    "        label = 'NVIDIA'\n",
    "        filenames.append(doc_label.split('_')[-1].split('.')[0])\n",
    "    labels.append(label)\n",
    "\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map labels to colors using a palette\n",
    "unique_labels = list(set(labels))  # Get unique labels\n",
    "color_palette = Category10[10]  # Use a palette with enough colors\n",
    "label_to_color = {label: color_palette[i] for i, label in enumerate(unique_labels)}  # Map labels to colors\n",
    "colors = [label_to_color[label] for label in labels]  # Assign colors to each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, max_iter=1000, random_state=42)\n",
    "tsne_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "tsne_title = \"t-SNE Visualization of Embeddings\"\n",
    "bokeh_plot(tsne_embeddings, filenames, labels, colors, tsne_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_reducer = umap.UMAP(n_neighbors=15, n_components=3, metric='euclidean')\n",
    "umap_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "umap_title = \"UMAP Visualization of Embeddings\"\n",
    "bokeh_plot(umap_embeddings, filenames, labels, colors, umap_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=\"nvdev/nvidia/llama-3.1-nemotron-70b-instruct\")\n",
    "context = load_documents(rfiDIR, [])\n",
    "\n",
    "task = llm.invoke(f\"Extract the problem statement, goals, constraints, \\\n",
    "and each task in the scope of work in the following: {context}. \\\n",
    "Do not summarize. Delimit each section with two empty lines. \\\n",
    "Do not use any markdown or bullets. Expand all acronyms expanded just after each one where found within parentheses if not already explained.\")\n",
    "\n",
    "questions = llm.invoke(f'Please extract all questions asked in the following as a numbered list: {context}. Do not include any headings or markdown. Include all acronyms expanded just after each one where found within parentheses if it has not already been explained.')\n",
    "\n",
    "if task.response_metadata['finish_reason']=='length':\n",
    "    print(\"WARNING: Response artificially truncated due to token length, please split up model calls to avoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(task.content))\n",
    "print(\"\")\n",
    "display(Markdown(questions.content))\n",
    "rfi_details = task.content + questions.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the RAG Pipeline\n",
    "\n",
    "Let's test our RAG pipeline with and without the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(model=\"nvdev/meta/llama-3.1-8b-instruct\")\n",
    "query = f\"You are a senior program manager at World Wide Technologies (WWT), incoporate WWT expertise and products where applicable to address the following Request for Information (RFI) with the following details: {task}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"No RAG:\\n\")\n",
    "display(Markdown(rag_query(query, llm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"RAG with vector store:\\n\")\n",
    "response, chunks = rag_query(query, llm, vectorstore, rerank=True, show_docs=True, tag_values=['Partner'])\n",
    "display(Markdown(response.content))\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Problem Statement**\n",
      "[Document(id='b3b9167d-f566-4afc-8f86-35dd2e5cca7a', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_game-engines_unity-engine.txt'}, page_content='27. https://unity.com/download 28. https://developer.nvidia.com/game-engines/unity-engine'), Document(id='d1932f27-570d-4c8b-b892-d1cf5375ba3d', metadata={'source': '../data/partner-docs/Part 3_ Inside RFP Assistant — Generating a Full RFP Response - WWT.pdf', 'page': 8, 'page_label': '9'}, page_content='https://www.wwt.com/wwt-research/part-3-inside-rfp-assistant-generating-a-full-rfp-response 9/10'), Document(id='3620f6b7-748c-4aac-8299-e8f32763f207', metadata={'source': '../data/partner-docs/Part 3_ Inside RFP Assistant — Generating a Full RFP Response - WWT.pdf', 'page': 6, 'page_label': '7'}, page_content='https://www.wwt.com/wwt-research/part-3-inside-rfp-assistant-generating-a-full-rfp-response 7/10'), Document(id='0d3e11c0-4d6c-4ce7-a1d2-45de23daf824', metadata={'source': '../data/partner-docs/Part 2_ Inside RFP Assistant — Summarizer and Qualification Workflow - WWT.pdf', 'page': 0, 'page_label': '1'}, page_content='https://www.wwt.com/wwt-research/part-2-inside-rfp-assistant-summarizer-and-qualiﬁcation-workﬂow 1/13')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, it appears that World Wide Technologies (WWT) has developed a suite of generative AI tools, including RFP Assistant, which utilizes NVIDIA technologies to enhance its capabilities. \n",
       "\n",
       "To address the \"Problem Statement\" section, WWT might leverage their generative AI expertise in the following ways:\n",
       "\n",
       "1. **Automated Problem Identification**: Utilizing the summarizer and qualification workflow (1/13) of their RFP Assistant, WWT could develop an automated feature that identifies and categorizes potential issues or pain points associated with the problem statement.\n",
       "\n",
       "2. **Predictive Analytics**: By integrating NVIDIA technologies, WWT could infuse predictive analytics capabilities into their generative AI, enabling it to forecast potential consequences and outcomes related to the problem statement. This would help users better understand and quantify the impact of the problem.\n",
       "\n",
       "3. **Data-Driven Problem Definition**: The RFP Assistant's capability to generate a full RFP response (9/10) would be invaluable in this context. WWT could use this feature to develop an analysis that identifies key problem areas, which are then matched with relevant business solutions, thereby creating a well-defined problem statement.\n",
       "\n",
       "4. **Feasibility Studies**: Employing NVIDIA’s expertise in AI and machine learning, WWT could develop disruptive and human-like problem-solving AI that examines problem statements, performs analyses, identifies the most relevant components to address, and generates feasible solutions.\n",
       "\n",
       "5. **Holistic Problem Understanding**: The qualitative analysis inherent in their models could help developers understand the nuances of complex problems and uncover root causes. This would enable WWT to propose solutions that address not just the apparent problem but also its underlying causes.\n",
       "\n",
       "By applying these strategies, WWT can create an AI-powered tool that not only recognizes and clarifies the problem statement but also offers actionable insights and feasible solutions, becoming a valuable asset in both IT project planning and business decision-making processes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Underdeveloped requirements can lead to unsatisfactory outcomes in federal contracts, limiting the ability to meet mission needs. The generation of appropriate pre-acquisition documentation has made IT acquisitions increasingly complex, creating challenges for business owners who may not be familiar with the nuance of developing requirements. Ultimately, poorly-written or incomplete pre-acquisition documentation (e.g. Statement of Objectives (SOO), Statement of Work (SOW), Performance Work Statement (PWS), market research, etc.) creates delays in the acquisition lifecycle. Department of State (DoS) program offices with IT acquisition needs require a solution incorporating statutory and regulatory considerations when building requirements documentation.\n",
      "[Document(id='cfa5ced3-9db6-405b-8aa9-8de06640bd26', metadata={'source': '../data/partner-docs/How Atom Ai and RFP Assistant have Evolved and What it Means for the Future - WWT.pdf', 'page': 3, 'page_label': '4'}, page_content=\"Research and reference relevant case studies by technology and industry\\nCross-reference WWT-related or niche technical terms\\nDraft emails leveraging internal data that would normally require a huge manual\\nlift to find\\nBusiness goal evolution\\nThe original goal of the ATC Assistant was an intelligent application that could provide\\ndetailed and specific insights into WWT's wide range of proof of concept (POC)\\nresults. With its expanded access to wwt.com data, the underlying goal of Atom is to\\nincrease employee productivity and provide insights across five main use cases:\\n1. Research WWT's services and capabilities\\n2. Research past projects and case studies\\n3. Draft external-facing materials\\n4. Identify the point of contact and subject-matter experts for a certain domain\"), Document(id='4a455e34-2a40-4acc-9acd-50831421a975', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 0, 'page_label': '1'}, page_content='phases. \\n \\nProblem Statement \\nUnderdeveloped requirements can lead to unsatisfactory outcomes in federal contracts, limiting the ability to \\nmeet mission needs. The generation of appropriate pre-acquisition documentation has made IT acquisitions \\nincreasingly complex, creating challenges for business owners who may not be familiar with the nuance of \\ndeveloping requirements.     \\nUltimately, poorly-written or incomplete pre-acquisition documentation (e.g. SOO, SOW, PWS, market \\nresearch, etc.) creates delays in the acquisition lifecycle. DoS program offices with IT acquisition needs require \\na solution incorporating statutory and regulatory considerations when building requirements documentation. \\nProduct Vision'), Document(id='2b622feb-ed78-41e8-9771-30b0012a2874', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 1, 'page_label': '2'}, page_content='etc.). \\n \\nProject Objectives \\n1. Automate and leverage technology to streamline the creation of documentation for the PR package \\ncreation process. \\n2. Discover new models and solutions that may have relevant application to acquisitions. \\n3. Leverage machine learning to collate acquisition data and trends to enable data-driven decisions for \\nfuture acquisitions.  \\n4. The solution will provide low barrier to entry regardless of user technical capability. \\n \\nConstraints \\nThis list is not exhaustive, however, the major constraints as DoS sees them are:'), Document(id='7d6fe07e-5cf8-46e1-9862-29379396e9d2', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 1, 'page_label': '2'}, page_content='07/19/2023 \\n2  \\n \\nAssist in market research by identifying relevant acquisition strategies, including but not limited to: \\ncontracting vehicles, socioeconomic considerations, applicable statutory and regulatory supply chain \\nconsiderations, security requirements, and market trends impacting the procurement. \\n \\nIncreased Efficiency and Accuracy \\nReduce the time required for drafting requirement documentation and increase the accuracy of \\nincorporating applicable, regulatory IT requirements (including cybersecurity) and evaluation factors. The \\nconcept should incorporate industry best practices for IT procurements, enabling procurement teams to \\nintroduce innovation and focus on strategic tasks, such as supplier relationship management and contract \\nnegotiation.')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "World Wide Technologies (WWT) can leverage their generative AI expertise enhanced with NVIDIA technologies to address the issue of underdeveloped requirements in federal contracts by:\n",
       "\n",
       "1. **Automating the creation of documentation**: WWT's AI-powered tools can assist in generating high-quality pre-acquisition documentation such as SOO, SOW, PWS, and market research reports. This can be achieved by leveraging NVIDIA's deep learning capabilities to analyze and synthesize large datasets related to IT acquisitions, DoS regulations, and industry best practices.\n",
       "2. **Developing a knowledge graph**: WWT can create a knowledge graph that integrates DoS regulations, industry best practices, and IT acquisition data. This graph can be used to provide relevant insights, recommendations, and even draft pre-acquisition documentation for WWT's customers, such as DoS program offices.\n",
       "3. **Identifying relevant acquisition strategies**: WWT's AI-powered tools can analyze market trends, statutory and regulatory considerations, and supply chain requirements to provide actionable insights on relevant acquisition strategies, including contracting vehicles, socioeconomic considerations, and security requirements.\n",
       "4. **Ensuring compliance and accuracy**: WWT's generative AI can be trained on NVIDIA's GPUs to ensure that generated documentation and reports are accurate, compliant with DoS regulations, and incorporate latest industry best practices.\n",
       "5. **Enabling data-driven decision making**: By leveraging NVIDIA's analytics platform, WWT can provide DoS program offices with data-driven insights and recommendations on future acquisitions, enabling informed decisions that meet mission needs.\n",
       "6. **Streamlining the acquisition lifecycle**: WWT's AI-powered tools can automate tasks such as drafting requirement documentation, reducing the time and effort required for IT acquisitions, and increasing accuracy in incorporating regulatory requirements and evaluation factors.\n",
       "\n",
       "By applying WWT's generative AI expertise enhanced with NVIDIA technologies, the company can address the challenges associated with underdeveloped requirements in federal contracts, ultimately leading to faster and more accurate acquisition processes with reduced delays."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Goals**\n",
      "[Document(id='b3b9167d-f566-4afc-8f86-35dd2e5cca7a', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_game-engines_unity-engine.txt'}, page_content='27. https://unity.com/download 28. https://developer.nvidia.com/game-engines/unity-engine'), Document(id='d1932f27-570d-4c8b-b892-d1cf5375ba3d', metadata={'source': '../data/partner-docs/Part 3_ Inside RFP Assistant — Generating a Full RFP Response - WWT.pdf', 'page': 8, 'page_label': '9'}, page_content='https://www.wwt.com/wwt-research/part-3-inside-rfp-assistant-generating-a-full-rfp-response 9/10'), Document(id='3620f6b7-748c-4aac-8299-e8f32763f207', metadata={'source': '../data/partner-docs/Part 3_ Inside RFP Assistant — Generating a Full RFP Response - WWT.pdf', 'page': 6, 'page_label': '7'}, page_content='https://www.wwt.com/wwt-research/part-3-inside-rfp-assistant-generating-a-full-rfp-response 7/10'), Document(id='0d3e11c0-4d6c-4ce7-a1d2-45de23daf824', metadata={'source': '../data/partner-docs/Part 2_ Inside RFP Assistant — Summarizer and Qualification Workflow - WWT.pdf', 'page': 0, 'page_label': '1'}, page_content='https://www.wwt.com/wwt-research/part-2-inside-rfp-assistant-summarizer-and-qualiﬁcation-workﬂow 1/13')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, it seems that World Wide Technologies (WWT) has been researching the use of generative AI, specifically a tool called RFP Assistant, and how it can be enhanced with NVIDIA technologies.\n",
       "\n",
       "The **Goals** section of an RFP (Request for Proposal) Response is typically where the vendor outlines their objectives, scope, and expected outcomes for the project. Here's how WWT might leverage their generative AI expertise enhanced with NVIDIA technologies to address the Goals section:\n",
       "\n",
       "**Goals**\n",
       "\n",
       "With the goal of delivering innovative, efficient, and effective solutions, our team will focus on the following objectives:\n",
       "\n",
       "* **Enhance Collaboration**: Utilize NVIDIA's AI-powered tools to enable seamless collaboration between stakeholders, ensuring that everyone is aligned and working towards a common goal.\n",
       "* **Accelerate Go-To-Market**: Leverage NVIDIA's accelerated computing capabilities to rapidly generate high-quality content, including technical proposals, presentations, and other materials, allowing us to respond to RFPs in a timely and competitive manner.\n",
       "* **Improve Proposal Quality**: By utilizing WWT's RFP Assistant tool, powered by NVIDIA technologies, we will generate well-structured, concise, and data-driven responses that effectively address the requirements of the solicitation, increasing the likelihood of winning the contract.\n",
       "* **Streamline Workflow**: WWT's expertise in AI and NVIDIA technologies will help automate repetitive tasks, allowing our team to focus on high-value activities that drive business growth and innovation.\n",
       "* **Foster Innovation**: Embracing AI and NVIDIA technologies will enable us to explore new and innovative solutions that meet the needs of the client, setting us apart from our competitors and demonstrating our commitment to excellence.\n",
       "\n",
       "By incorporating these objectives, World Wide Technologies can effectively leverage their generative AI expertise enhanced with NVIDIA technologies to address the Goals section of an RFP Response, demonstrating their ability to deliver innovative, efficient, and effective solutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Improved Decision-Making\n",
      "2. Increased Efficiency and Accuracy\n",
      "3. Better Contract Outcomes\n",
      "4. Reduce Costs\n",
      "5. Compliance in Output\n",
      "[Document(id='e9b777b1-4a52-4284-89e0-f039dbc9029a', metadata={'source': '../data/partner-docs/Part 1_ Meet RFP Assistant — the AI Improving Proposal Response Speed and Efficiency - WWT.pdf', 'page': 8, 'page_label': '9'}, page_content='https://www.wwt.com/wwt-research/part-1-meet-rfp-assistant-the-ai-improving-proposal-response-speed-and-eﬃciency 9/21'), Document(id='0d3e11c0-4d6c-4ce7-a1d2-45de23daf824', metadata={'source': '../data/partner-docs/Part 2_ Inside RFP Assistant — Summarizer and Qualification Workflow - WWT.pdf', 'page': 0, 'page_label': '1'}, page_content='https://www.wwt.com/wwt-research/part-2-inside-rfp-assistant-summarizer-and-qualiﬁcation-workﬂow 1/13'), Document(id='15dda612-b7d3-4195-b824-ecb2f6ecf840', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_drive_drive-sim.txt'}, page_content='5. https://www.nvidia.com/en-us/omniverse/cloud/#next-steps 6. https://developer.nvidia.com/drive/simulation'), Document(id='dc527b84-3310-4c6e-960b-faf09c1b2f33', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_rtx_ray-tracing_optix.txt'}, page_content='74. https://developer.nvidia.com/designworks/optix/download 75. https://developer.nvidia.com/rtx/ray-tracing/optix')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, here's how World Wide Technologies (WWT) might leverage their generative AI expertise enhanced with NVIDIA technologies to address the mentioned sections:\n",
       "\n",
       "**Improved Decision-Making**\n",
       "\n",
       "* WWT's RFP Assistant can be integrated with NVIDIA's Omniverse platform to create a converged workspace for stakeholders to interact and share information in real-time. This will enable faster decision-making through augmented collaboration and visibility.\n",
       "* The RFP Assistant's AI-driven summarization and qualification workflows, developed in partnership with NVIDIA, can be used to quickly understand complex procurement data, making it easier for stakeholders to identify key insights and make informed decisions.\n",
       "\n",
       "**Increased Efficiency and Accuracy**\n",
       "\n",
       "* WWT can utilize NVIDIA's OptiX technology to accelerate compute-intensive tasks, such as processing large datasets and performing simulations, which is particularly relevant in complex procurement scenarios. This will enable the RFP Assistant to respond more quickly and accurately to users' queries.\n",
       "* By leveraging NVIDIA's design and simulation tools (e.g., DesignWorks), WWT can create more realistic and accurate visual models of procurement scenarios, further enhancing the decision-making process and reducing the risk of errors.\n",
       "\n",
       "**Better Contract Outcomes**\n",
       "\n",
       "* WWT can apply their generative AI expertise to automate contract writing and review processes, reducing the risk of errors, miscommunication, and misinterpretation. This can be achieved by integrating their RFP Assistant with NVIDIA's Drive Simulation platform, which enables the simulation of complex procurement scenarios and outcomes.\n",
       "* By modeling different contract outcomes and their corresponding risks, WWT's RFP Assistant can provide more accurate and reliable contract simulations, allowing stakeholders to make better-informed decisions.\n",
       "\n",
       "**Reduce Costs**\n",
       "\n",
       "* WWT's RFP Assistant can leverage NVIDIA's Cloud services to reduce infrastructure costs associated with large-scale data processing and simulation. This will enable WWT to deploy their generative AI expertise to a broader range of procurement scenarios, without incurring significant capital expenditures.\n",
       "* By optimizingProcesses and automating routine tasks, WWT can streamline their internal workflows and reduce labor costs, further enabling them to pass the cost savings on to clients.\n",
       "\n",
       "**Compliance in Output**\n",
       "\n",
       "* WWT can utilize NVIDIA's OptiX technology to perform automated quality control and review processes on output generated by their RFP Assistant, ensuring that all outputs meet regulatory and contractual requirements.\n",
       "* By integrating their RFP Assistant with NVIDIA's DesignWorks, WWT can ensure that all contracts and related documentation comply with relevant regulations and directives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Constraints**\n",
      "[Document(id='b3b9167d-f566-4afc-8f86-35dd2e5cca7a', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_game-engines_unity-engine.txt'}, page_content='27. https://unity.com/download 28. https://developer.nvidia.com/game-engines/unity-engine'), Document(id='3620f6b7-748c-4aac-8299-e8f32763f207', metadata={'source': '../data/partner-docs/Part 3_ Inside RFP Assistant — Generating a Full RFP Response - WWT.pdf', 'page': 6, 'page_label': '7'}, page_content='https://www.wwt.com/wwt-research/part-3-inside-rfp-assistant-generating-a-full-rfp-response 7/10'), Document(id='d1932f27-570d-4c8b-b892-d1cf5375ba3d', metadata={'source': '../data/partner-docs/Part 3_ Inside RFP Assistant — Generating a Full RFP Response - WWT.pdf', 'page': 8, 'page_label': '9'}, page_content='https://www.wwt.com/wwt-research/part-3-inside-rfp-assistant-generating-a-full-rfp-response 9/10'), Document(id='0d3e11c0-4d6c-4ce7-a1d2-45de23daf824', metadata={'source': '../data/partner-docs/Part 2_ Inside RFP Assistant — Summarizer and Qualification Workflow - WWT.pdf', 'page': 0, 'page_label': '1'}, page_content='https://www.wwt.com/wwt-research/part-2-inside-rfp-assistant-summarizer-and-qualiﬁcation-workﬂow 1/13')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, it appears that World Wide Technologies (WWT) has been exploring the use of generative AI, specifically RFP (Request for Proposal) assistance tools, enhanced with NVIDIA technologies. To address the section on \"Constraints,\" WWT might leverage their expertise in the following ways:\n",
       "\n",
       "1. **Identifying and mitigating constraints**: Using their summarizer and qualification workflow (mentioned in part 2 of the research), WWT could analyze the constraints section to identify the specific limitations, risks, or concerns that need to be addressed.\n",
       "2. **Generating constraints-aware responses**: The RFP assistant, powered by NVIDIA technologies, could generate responses that take into account the identified constraints. This might involve suggesting creative solutions or compromises that balance the client's needs with the project's limitations.\n",
       "3. **Visualizing constraints**: By integrating NVIDIA's graphics processing unit (GPU) capabilities, WWT could develop interactive visualizations that help stakeholders understand the constraints and their implications on the project. This might include 3D models, heat maps, or other data visualizations.\n",
       "4. **Simulating constraints scenarios**: Leveraging NVIDIA's simulation tools and frameworks, WWT could create simulated environments that test the project's response to various constraints scenarios. This would enable the development team to anticipate potential challenges and design more robust solutions.\n",
       "5. **Collaborative constraint mapping**: Using NVIDIA's GRID technology, WWT could facilitate real-time, collaborative constraint mapping sessions. This would enable stakeholders from different teams to contribute to and refine the constraints section in a single, shared environment.\n",
       "6. **Constraint-agnostic templates**: WWT could develop a set of pre-built, constraint-agnostic templates that can be easily adapted to specific projects. These templates would be informed by their generative AI expertise and NVIDIA technologies, ensuring that the constraints section is comprehensive and well-structured.\n",
       "7. **Constraint-resolution workflows**: Building on their summarizer and qualification workflow, W WT could design workflows that incorporate NVIDIA technologies to automate parts of the constraint analysis and resolution process. This would help streamline the RFP response development and minimize the risk of overlooking critical constraints.\n",
       "\n",
       "By applying these strategies, World Wide Technologies (WWT) could effectively address the section on \"Constraints\" and demonstrate their expertise in leveraging NVIDIA technologies to enhance their generative AI solutions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Generative AI tools (e.g. ChatGPT4, Large Language Models (LLMs) on Hugging Face) are not approved for use on Department networks.\n",
      "2. The Information Technology Configuration Control Board (ITCCB) and Authority to Operate (ATO) processes are time-consuming (up to 12 months).\n",
      "3. Most government employees involved are Acquisitions Subject Matter Experts (SMEs), not Artificial Intelligence (AI) SMEs, and cannot shepherd a contractor through ITCCB or ATO processes.\n",
      "4. Complete Purchase Request (PR) packages must be submitted through the agency contract writing system platform for Acquisition and Procurement Executive (A/OPE)/Acquisition Quality Management (AQM) action.\n",
      "5. Limited DoS IT acquisitions examples for training data; stakeholders must agree on data quality.\n",
      "6. Cloud products require Federal Risk and Authorization Management Program (FedRAMP) authorization.\n",
      "7. No stakeholders can sponsor the FedRAMP process.\n",
      "8. Development and implementation require contractors with AI engineering expertise and necessitate ongoing funding for maintenance and additional features; outside contractors may bias the solution technology.\n",
      "9. AI must not perform inherently governmental functions as described in Federal Acquisition Regulation (FAR) 7.5.\n",
      "10. The solution must avoid vendor \"lock-in,\" with the large language module being replaceable with minimal effort.\n",
      "11. Limited DoS common infrastructure for collaboration and execution with partners across the Department.\n",
      "12. Compliance with Executive Order (E.O.) 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government and 20 Foreign Affairs Manual (FAM) 201.1.\n",
      "13. Users come from varying technical backgrounds, which may limit challenges interacting with the model.\n",
      "14. Pending legislation regarding AI model training may increase timelines and costs.\n",
      "[Document(id='8ebfdf00-03f6-47a0-b052-f4ee00d7fbce', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 2, 'page_label': '3'}, page_content='07/19/2023 \\n3  \\n \\n1. Generative AI tools such as ChatGPT4 and other Large Language Models (LLM) found on \\nhuggingface.com (and industry aggregator) have not been approved for use on Department networks. \\nA list of AI projects currently in use by DoS can be found here: https://www.state.gov/data-\\nstrategy/ai_inventory/ \\n2. The Information Technology Configuration Control Board (ITCCB) and ATO processes are time \\nconsuming, taking up to 12 months to complete. \\n3. Most government employees involved in this project are acquisitions SMEs not AI SMEs and will not be \\nable to shepherd a contractor through the ITCCB or ATO processes. \\n4. Complete PR packages must be submitted through the agency contract writing system platform for \\nA/OPE/AQM action.'), Document(id='79097068-6e55-49f6-a6f0-ae14ddea38e1', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_topics_ai_.txt'}, page_content='1. Topics 2. Artificial Intelligence Artificial Intelligence Explore all that NVIDIA offers developers working with AI—from data processing and ETL feature engineering to graph, classical machine learning, and deep learning model training to large-scale inference. Catalog: API Catalog : * Documentation * Forum * Technical Blog * On-Demand Sessions Explore AI Topics Explore All Topics NVIDIA LLM developers web journey Generative AI Train massive multimodal models, fine-tune for use cases, and quantize and deploy from data centers to the smallest embedded devices. Explore Generative AI Resources Using cybersecurity resources to detect threats within large amounts of data Data Science Learn techniques and strategies to accelerate your data pipelines. Explore Data Science Resources Using AI'), Document(id='8df88d6e-987d-48b8-9099-425f943c0225', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_nvidia-nemo.txt'}, page_content='can choose to engage with the NVIDIA AI platform at any layer of the stack, from infrastructure, software, and models to applications, either directly through NVIDIA products or through a vast ecosystem of offerings. Start With State-of-the-Art Foundation Models Try the latest models, including Llama 3, NVIDIA Nemotron™ family, NVIDIA Cosmos™ World Foundation Models, and more. Experience AI Foundation Models Deploy AI Models Across Platforms Quickly deploy AI models using easy-to-use inference microservices. Deploy With NVIDIA NIM Build World Models for Physical AI Use state-of-the-art generative world foundation models and data processing pipelines to accelerate physical AI systems. Get Started With Cosmos Connect Generative AI Models to Knowledge Bases Use retrieval-augmented generation'), Document(id='956d7a61-b189-491f-9aaf-73884761dd17', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_generative-ai.txt'}, page_content='can choose to engage with the NVIDIA AI platform at any layer of the stack, from infrastructure, software, and models to applications, either directly through NVIDIA products or through a vast ecosystem of offerings. Start With State-of-the-Art Foundation Models Try the latest models, including Llama 3, NVIDIA Nemotron™ family, NVIDIA Cosmos™ World Foundation Models, and more. Experience AI Foundation Models Deploy AI Models Across Platforms Quickly deploy AI models using easy-to-use inference microservices. Deploy With NVIDIA NIM Build World Models for Physical AI Use state-of-the-art generative world foundation models and data processing pipelines to accelerate physical AI systems. Get Started With Cosmos Connect Generative AI Models to Knowledge Bases Use retrieval-augmented generation')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Leveraging Generative AI Expertise with NVIDIA Technologies: Addressing Department of State Challenges**\n",
       "\n",
       "World Wide Technologies (WWT) can harness their generative AI expertise enhanced with NVIDIA technologies to address the Department of State's (DoS) challenges in AI adoption. By partnering with NVIDIA, WWT can provide a robust, secure, and scalable AI platform that addresses the DoS's specific needs.\n",
       "\n",
       "**Addressing Key Challenges:**\n",
       "\n",
       "1.  **Generative AI Tool Approval:**\n",
       "\n",
       "    *   WWT and NVIDIA can help facilitate the evaluation and approval process for generative AI tools like ChatGPT4 and LLMs on Hugging Face, ensuring compliance with Department networks.\n",
       "2.  **Time-Consuming ITCCB and ATO Processes:**\n",
       "\n",
       "    *   WWT and NVIDIA can offer accelerated ITCCB and ATO processes, utilizing their expertise to shepherd contractors through the processes, reducing timelines from 12 months to 6 months or less.\n",
       "3.  **Acquisitions SME Limitations:**\n",
       "\n",
       "    *   By leveraging NVIDIA's extensive AI expertise, WWT can train and equip Acquisitions SMEs to handle AI-related tasks, ensuring they can guide contractors effectively.\n",
       "4.  **AI-Driven Solution Development:**\n",
       "\n",
       "    *   WWT's collaboration with NVIDIA will result in the creation of AI-driven solutions, allowing stakeholders to contribute to training data quality and ensuring the solution meets the DoS's requirements.\n",
       "5.  **FedRAMP Authorization and Additional Support:**\n",
       "\n",
       "    *   WWT and NVIDIA can assist with the FedRAMP authorization process, eliminating the need for DoS stakeholders to sponsor the process.\n",
       "6.  **Secure, Scalable, and Vendor-Agnostic Solution:**\n",
       "\n",
       "    *   WWT's AI engineering expertise, combined with NVIDIA's technology, will develop a solution that supports large language module replacement without significant effort, ensuring flexibility and avoidance of vendor \"lock-in.\"<\n",
       "7.  **Compliance with Executive Orders and Foreign Affairs Policies:**\n",
       "\n",
       "    *   The solution will adhere to E.O. 13960 and 20 FAM 201.1, ensuring a trustworthy AI approach in the Federal government.\n",
       "8.  **Intuitive Model Interaction for Users:**\n",
       "\n",
       "    *   WWT and NVIDIA will design a user-friendly interface for interacting with the model, providing users from varying technical backgrounds with a seamless experience.\n",
       "9.  **Accommodating AI Model Training Challenges:**\n",
       "\n",
       "    *   The joint initiative will stay up-to-date with the status of pending legislation, planning for changes that may increase timelines and costs.\n",
       "\n",
       "**Preserving Alignment with DoS Priorities**\n",
       "\n",
       "Throughout this approach, we will maintain a shared understanding of the priorities driving DoS's interests, collaboration with NVIDIA, and options identified in the generative AI market to ensure effective, in-market solutions.\n",
       "\n",
       "**A New Standard for Generative AI in the Public Sector**\n",
       "\n",
       "World Wide Technologies (WWT) can bridge the gap between generative AI expertise and AI readiness in the public sector by using NVIDIA AI technologies. This technical strategy preserves alignment with the priorities driving DoS interests, advancing the use of trustworthy, federally compliant AI in the public sector."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**Tasks within the Scope of Work**\n",
      "[Document(id='15dda612-b7d3-4195-b824-ecb2f6ecf840', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_drive_drive-sim.txt'}, page_content='5. https://www.nvidia.com/en-us/omniverse/cloud/#next-steps 6. https://developer.nvidia.com/drive/simulation'), Document(id='0d3e11c0-4d6c-4ce7-a1d2-45de23daf824', metadata={'source': '../data/partner-docs/Part 2_ Inside RFP Assistant — Summarizer and Qualification Workflow - WWT.pdf', 'page': 0, 'page_label': '1'}, page_content='https://www.wwt.com/wwt-research/part-2-inside-rfp-assistant-summarizer-and-qualiﬁcation-workﬂow 1/13'), Document(id='3620f6b7-748c-4aac-8299-e8f32763f207', metadata={'source': '../data/partner-docs/Part 3_ Inside RFP Assistant — Generating a Full RFP Response - WWT.pdf', 'page': 6, 'page_label': '7'}, page_content='https://www.wwt.com/wwt-research/part-3-inside-rfp-assistant-generating-a-full-rfp-response 7/10'), Document(id='b3b9167d-f566-4afc-8f86-35dd2e5cca7a', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_game-engines_unity-engine.txt'}, page_content='27. https://unity.com/download 28. https://developer.nvidia.com/game-engines/unity-engine')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, it appears that World Wide Technologies (WWT) is leveraging their generative AI expertise and NVIDIA technologies to improve their Response for Proposal (RFP) Assistant workflow. To highlight how WWT might leverage their expertise to address the \"Tasks within the Scope of Work\" section, I'll outline some possible applications:\n",
       "\n",
       "1. **Automated Request for Information (RFI) Response Generation**: Using NVIDIA's AI technologies, such as the Omniverse Cloud platform and NVIDIA Drive Simulation, WWT could create a system that analyzes RFI questions and generates accurate, detailed responses. This would save time and effort for their team, while also ensuring consistency and accuracy in their RFI responses.\n",
       "2. **Dynamic RFP Response Generation**: By integrating their generative AI expertise with NVIDIA's resources, WWT could develop a system that generates full RFP responses in a matter of minutes, using a modular, template-based approach. This would enable their team to quickly adjust to changing RFP requirements and provide high-quality responses.\n",
       "3. **Assessing and Prioritizing Tasks within the Scope of Work**: NVIDIA's AI technologies could help WWT develop a system that analyzes the Scope of Work (SOW) document, identifies key tasks and requirements, and assigns priorities based on decreasing uncertainty. This would allow their team to focus on the most critical tasks first, ensuring timely and high-quality delivery.\n",
       "4. **Automated Proforma Invoice and Pricing Calculation**: By integrating their AI expertise with NVIDIA's technologies, WWT could create a system that generates proforma invoices and calculates prices based on the Scope of Work, existing agreements, and historical data. This would reduce manual errors and improve the accuracy of financial calculations.\n",
       "5. **Real-time Collaboration and Workflow Management**: Using NVIDIA's Omniverse Cloud platform, WWT could develop a collaborative environment that enables team members to work together on RFP responses, even when remote. This would streamline the_workflow, improve communication, and reduce misunderstandings.\n",
       "\n",
       "Some potential highlights for demonstrating these capabilities could include:\n",
       "\n",
       "* **Real-world examples**: Showcase successful projects or RFP responses that benefited from WWT's AI-enhanced process, highlighting the efficiency and accuracy gained.\n",
       "* **Benchmarks and performance metrics**: Provide data on the impact of AI-enhanced processes on response generation speed, accuracy, and quality.\n",
       "* **Third-party validation**: Include testimonials or case studies from clients who've benefited from WWT's AI-driven RFP support services.\n",
       "* **Technological affinity illustrations**: Visualize the integration of NVIDIA technologies and WWT's expertise through imagery, diagrams, or animations, demonstrating the significant benefits that can be achieved by combining their capabilities.\n",
       "\n",
       "The illustration points above can help identify how WWT is improving the collaborative and conversational understanding of AI, RFP technologies, and provide data that will make the key point effectively for the public."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Identify the Procurement Processes to Automate**\n",
      "   Analyze procurement data to identify processes where Artificial Intelligence (AI)/Machine Learning (ML) capabilities can improve efficiency and decision-making. Specify parts of the procurement process requiring human review due to involving inherently governmental work, compliance with Federal AI policies and frameworks, or an insurmountable cost to automate.\n",
      "[Document(id='2dae9818-21f3-46da-a1c9-a3105902e858', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 3, 'page_label': '4'}, page_content='Identify the Procurement Processes to Automate  \\nAnalyze the procurement data and identify processes implementing AI/ML capabilities improve \\nefficiency and decision-making. Further, identify and specify which parts of the procurement process \\nrequire human review due to involving inherently governmental work, compliance with Federal AI \\npolicies and frameworks, or an insurmountable cost to automate.   \\n \\nChoose the Right AI Tools \\nThe choice of AI tools will depend on the specific procurement processes automated, the desired \\noutcomes, and the cybersecurity constraints involved with building anything on DoS OpenNet.'), Document(id='09b3d2e1-85db-4b9d-a848-84a2bce9895c', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 0, 'page_label': '1'}, page_content='Enforcement, (INL/EX/GAP), and the Bureau of Administration, Office of the Procurement Executive (A/OPE) \\nare conducting introductory market research into generative Artificial Intelligence (AI)/Machine Learning (ML) \\ncapabilities for the Department’s IT acquisitions requirement writing. \\nPre-acquisition business operations rely on dated technology and manually intensive processes resulting in \\nunexploited data resources, wasted labor hours, and inefficiencies. The goal of embedding AI technology into \\nan existing and recurring process is to reduce inefficiencies from manual laborious tasks, simplifying \\nworkflows, and improving the accuracy of repetitive tasks in the market research and acquisition planning \\nphases. \\n \\nProblem Statement'), Document(id='9649b378-b74c-4b4c-ad1d-523f7896d17f', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 0, 'page_label': '1'}, page_content='Product Vision \\nThe ideal concept will prompt requirements writers to input a problem statement based on program need. in \\nthe prompt would then generate a complete, draft pre-acquisition purchase request (PR) package for a \\ngovernment procurement professional to review and edit prior to submission in the agency contracting \\nwriting system.  \\n \\nProduct Goals \\nImproved Decision-Making'), Document(id='7d6fe07e-5cf8-46e1-9862-29379396e9d2', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 1, 'page_label': '2'}, page_content='07/19/2023 \\n2  \\n \\nAssist in market research by identifying relevant acquisition strategies, including but not limited to: \\ncontracting vehicles, socioeconomic considerations, applicable statutory and regulatory supply chain \\nconsiderations, security requirements, and market trends impacting the procurement. \\n \\nIncreased Efficiency and Accuracy \\nReduce the time required for drafting requirement documentation and increase the accuracy of \\nincorporating applicable, regulatory IT requirements (including cybersecurity) and evaluation factors. The \\nconcept should incorporate industry best practices for IT procurements, enabling procurement teams to \\nintroduce innovation and focus on strategic tasks, such as supplier relationship management and contract \\nnegotiation.')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "World Wide Technologies (WWT) can leverage their generative AI expertise enhanced with NVIDIA technologies to analyze procurement data and identify processes where AI/ML capabilities can improve efficiency and decision-making. Here's how they can address the specified section:\n",
       "\n",
       "**Identify the Procurement Processes to Automate**\n",
       "\n",
       "Given the context, WWT can start by analyzing procurement data to identify areas that are labor-intensive, prone to errors, or slow down the procurement process. They can use NVIDIA technologies such as GPUs or deep learning frameworks like TensorFlow or PyTorch to facilitate the analysis.\n",
       "\n",
       "Some potential procurement processes that can be automated using AI/ML include:\n",
       "\n",
       "1. **Requirement gathering and documentation**: WWT can develop a generative AI model that uses natural language processing (NLP) and machine learning algorithms to help requirements writers produce draft pre-acquisition purchase request (PR) packages.\n",
       "2. **Market research and analysis**: WWT can create an AI-powered tool that identifies relevant acquisition strategies, including socioeconomic considerations, statutory and regulatory supply chain considerations, security requirements, and market trends impacting the procurement.\n",
       "3. **Contract analysis and recommendations**: WWT can develop an AI-driven platform that analyzes contract data, identifies best practices, and provides recommendations for future contracts.\n",
       "\n",
       "**Specifying parts of the procurement process requiring human review due to inherence governmental work, compliance with Federal AI policies and frameworks, or an insurmountable cost to automate**\n",
       "\n",
       "After identifying the potential procurement processes to automate, WWT can specify parts of the process requiring human review. These areas may include:\n",
       "\n",
       "1. **Inherently governmental work**: Tasks such as risk assessment, strategic decision-making, or review of complex technical requirements may require human oversight to ensure that they comply with government policies and regulations.\n",
       "2. **Compliance with Federal AI policies and frameworks**: WWT can identify areas that require strict adherence to Federal AI policies and frameworks, such as ensuring the use of bias-free algorithms or complying with data reside requirements.\n",
       "3. **Insurmountable costs to automate**: Where costs associated with implementing AI solutions outweigh the benefits, human review may be required to ensure that decisions are made with the best available information.\n",
       "\n",
       "In this context, WWT can continue to provide expertise to assess these risks and costs associated with automating procurement process, ensuring that any proposed solution is sound and effectively identifies benefits of automation, while providing flow-overs to human oversight processes where required."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. **Choose the Right AI Tools**\n",
      "   Select AI tools based on the specific procurement processes automated, desired outcomes, and cybersecurity constraints involved with building on DoS OpenNet.\n",
      "[Document(id='09b3d2e1-85db-4b9d-a848-84a2bce9895c', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 0, 'page_label': '1'}, page_content='Enforcement, (INL/EX/GAP), and the Bureau of Administration, Office of the Procurement Executive (A/OPE) \\nare conducting introductory market research into generative Artificial Intelligence (AI)/Machine Learning (ML) \\ncapabilities for the Department’s IT acquisitions requirement writing. \\nPre-acquisition business operations rely on dated technology and manually intensive processes resulting in \\nunexploited data resources, wasted labor hours, and inefficiencies. The goal of embedding AI technology into \\nan existing and recurring process is to reduce inefficiencies from manual laborious tasks, simplifying \\nworkflows, and improving the accuracy of repetitive tasks in the market research and acquisition planning \\nphases. \\n \\nProblem Statement'), Document(id='8e9400aa-61d7-477b-a969-190fd5e26571', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 4, 'page_label': '5'}, page_content='07/19/2023 \\n5  \\n \\nTrain the AI Algorithms \\nIdentify and ascertain ownership of any datasets  to train the model to be able to draw from existing \\nFederal Acquisition Regulations, cybersecurity requirements, DoS-specific contracting requirements, \\ncountry-specific constraints, exemplar requests for proposals, as well as other data, when generating \\nstylistically and structurally appropriate responses and draft documents. Further, train the algorithm \\nfor content, style, structure, and compliance. \\n \\nIntegrate AI with Existing Procurement Systems   \\nEnsure AI algorithms can access the data existing in relevant systems (e.g., ILMS/ARIBA, and Sam.gov, \\netc.) and that the created tool outputs can integrate seamlessly back into those systems. \\n \\nMonitor and Refine'), Document(id='5fc2f3f2-1535-4a67-aa99-b0bee572fe3d', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_higher-education-and-research.txt'}, page_content='opened unprecedented opportunities for businesses and the scientific community. In this panel session, learn how to take the next step in your career and explore ways to ensure you have the skills you need to tackle new workloads. Watch Session Webinars Essential Training and Tips to Accelerate Your Career in AI  us for a panel discussion on fostering career growth and learning in the fields of AI and other advanced technologies. Our panelists will share their unique career journeys, valuable insights, and tips and tricks for success. Watch Now Bringing Generative AI to Life With NVIDIA Jetson Experience next-gen applications emerging in robotics and computer vision by deploying foundational large language models (LLMs) and vision transformers into real-world embedded systems at the edge.'), Document(id='99cbb9db-3498-4395-a208-3a8bbc1ba371', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_clara-guardian.txt'}, page_content='SDK Latest News *  about the latest developer news *  about technical deep dive on Fleet Command Deep Learning Institute Training * Optimization and Deployment of TensorFlow Models with TensorRT * Fundamentals of Deep Learning for Computer Vision * Getting Started with AI on Jetson Nano Intelligent Video Analytics * NVIDIA DeepStream SDK for AI-based multi-sensor processing and video and image understanding * TAO Toolkit to create highly accurate AI models with zero coding Speech and NLP * NVIDIA NeMo, an open-source toolkit for building conversational AI models * NVIDIA Riva SDK for deploying conversational AI models that fuse vision, speech, and other sensor data Edge Hardware Explore the Fleet Command to securely manage and scale AI deployments.  Disclaimer: Clara SDKs and samples are')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "World Wide Technologies (WWT) can leverage their generative AI expertise enhanced with NVIDIA technologies to address the requirement by using the following approach:\n",
       "\n",
       "**Identify and Choose the Right AI Tools**\n",
       "\n",
       "WWT will utilize their expertise to evaluate and select the most suitable AI tools based on the specific procurement process automation requirements, desired outcomes, and cybersecurity constraints involved with building on DoS OpenNet. This includes:\n",
       "\n",
       "1.  **Assessing AI Tool Capabilities**: WWT will use NVIDIA tools and expertise to assess the capabilities of various AI tools and determine which ones can be effectively integrated with existing procurement systems, such as ILMS/ARIBA, and Sam.gov.\n",
       "2.  **Evaluating AI Tool Security**: WWT will evaluate the security features and constraints of each AI tool to ensure seamless integration with DoS OpenNet and adherence to stringent cybersecurity requirements.\n",
       "3.  **Selecting AI Tools**: Based on the assessment and evaluation, WWT will select the best-suited AI tools that meet the specific procurement process automation requirements, desired outcomes, and cybersecurity constraints.\n",
       "\n",
       "**Integrate Chosen AI Tools with Procurement Systems**\n",
       "\n",
       "Once the suitable AI tools are selected, WWT will work closely with NVIDIA experts to ensure the seamless integration of the chosen AI tools with existing procurement systems like ILMS/ARIBA, Sam.gov, and DoS OpenNet. This includes:\n",
       "\n",
       "1.  **API Integration**: WWT will use NVIDIA-provided APIs and tools to integrate the chosen AI tools with the procurement systems, enabling the generation of accurate and compliant documents.\n",
       "2.  **Testing and Quality Assurance**: WWT will conduct comprehensive testing and quality assurance to ensure the integrated AI tools meet the required outcomes and security standards.\n",
       "\n",
       "**Monitor and Refine the AI Solution**\n",
       "\n",
       "WWT will collaborate with NVIDIA experts to continuously monitor and refine the AI solution to optimize its performance, accuracy, and compliance with evolving requirements and constraints. This includes:\n",
       "\n",
       "1.  **Ongoing Monitoring**: WWT will ensure the AI solution continually monitors and adapts to changing procurement processes, desired outcomes, and cybersecurity requirements.\n",
       "2.  **Performance Optimization**: WWT will work with NVIDIA experts to optimize the AI solution's performance, improving response time, and accuracy, and reducing manual labor needs.\n",
       "\n",
       "By leveraging their generative AI expertise enhanced with NVIDIA technologies, WWT can deliver an AI-powered procurement solution that addresses the requirements and constraints outlined, providing significant improvements in efficiency, accuracy, and overall value to the Department."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. **Train the AI Algorithms**\n",
      "   Identify and ascertain ownership of datasets to train the model, drawing from existing Federal Acquisition Regulations, cybersecurity requirements, DoS-specific contracting requirements, country-specific constraints, exemplar requests for proposals, and other data. Train the algorithm for content, style, structure, and compliance.\n",
      "[Document(id='8e9400aa-61d7-477b-a969-190fd5e26571', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 4, 'page_label': '5'}, page_content='07/19/2023 \\n5  \\n \\nTrain the AI Algorithms \\nIdentify and ascertain ownership of any datasets  to train the model to be able to draw from existing \\nFederal Acquisition Regulations, cybersecurity requirements, DoS-specific contracting requirements, \\ncountry-specific constraints, exemplar requests for proposals, as well as other data, when generating \\nstylistically and structurally appropriate responses and draft documents. Further, train the algorithm \\nfor content, style, structure, and compliance. \\n \\nIntegrate AI with Existing Procurement Systems   \\nEnsure AI algorithms can access the data existing in relevant systems (e.g., ILMS/ARIBA, and Sam.gov, \\netc.) and that the created tool outputs can integrate seamlessly back into those systems. \\n \\nMonitor and Refine'), Document(id='2fd7af46-dbdc-47b1-9056-000cdb12b292', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_metropolis.txt'}, page_content='vision AI applications for the edge are challenged by more complex and longer development cycles. NVIDIA Metropolis offers a collection of powerful APIs and microservices for developers to easily develop and deploy applications on the edge to any cloud.  Powerful Tools for AI-Enabled Video Analytics The Metropolis suite of SDKs provides a variety of starting points for AI application development and deployment. * View All * Generate * Train * Build * Deploy NVIDIA Omniverse™ Replicator Generate physically accurate 3D synthetic data at scale, or build your own synthetic data tools and frameworks. Bootstrap perception AI model training and achieve accurate Sim2Real performance without having to manually curate and label real-world data.  About Omniverse Replicator Also check out'), Document(id='5fc2f3f2-1535-4a67-aa99-b0bee572fe3d', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_higher-education-and-research.txt'}, page_content='opened unprecedented opportunities for businesses and the scientific community. In this panel session, learn how to take the next step in your career and explore ways to ensure you have the skills you need to tackle new workloads. Watch Session Webinars Essential Training and Tips to Accelerate Your Career in AI  us for a panel discussion on fostering career growth and learning in the fields of AI and other advanced technologies. Our panelists will share their unique career journeys, valuable insights, and tips and tricks for success. Watch Now Bringing Generative AI to Life With NVIDIA Jetson Experience next-gen applications emerging in robotics and computer vision by deploying foundational large language models (LLMs) and vision transformers into real-world embedded systems at the edge.'), Document(id='99cbb9db-3498-4395-a208-3a8bbc1ba371', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_clara-guardian.txt'}, page_content='SDK Latest News *  about the latest developer news *  about technical deep dive on Fleet Command Deep Learning Institute Training * Optimization and Deployment of TensorFlow Models with TensorRT * Fundamentals of Deep Learning for Computer Vision * Getting Started with AI on Jetson Nano Intelligent Video Analytics * NVIDIA DeepStream SDK for AI-based multi-sensor processing and video and image understanding * TAO Toolkit to create highly accurate AI models with zero coding Speech and NLP * NVIDIA NeMo, an open-source toolkit for building conversational AI models * NVIDIA Riva SDK for deploying conversational AI models that fuse vision, speech, and other sensor data Edge Hardware Explore the Fleet Command to securely manage and scale AI deployments.  Disclaimer: Clara SDKs and samples are')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's how World Wide Technologies (WWT) might leverage their generative AI expertise enhanced with NVIDIA technologies to address the section on **Training the AI Algorithms**:\n",
       "\n",
       "**Goal:** Train AI algorithms to accurately generate responses and documents while ensuring compliance with various regulations and requirements.\n",
       "\n",
       "**NVIDIA Technologies Needed:**\n",
       "\n",
       "1. **NVIDIA Omniverse Replicator**: Generate high-quality, synthetic data to train AI models, reducing the need for manual data curation and labeling.\n",
       "2. **TAO Toolkit**: Create highly accurate AI models with zero coding, leveraging pre-trained models and fine-tuning them for specific use cases.\n",
       "3. **NVIDIA DeepStream SDK**: Integrate AI-powered video and image understanding capabilities to analyze data and optimize model performance.\n",
       "\n",
       "**WWT's Approach:**\n",
       "\n",
       "1. **Source Dataset**: Identify and obtain ownership of relevant datasets, including existing Federal Acquisition Regulations, cybersecurity requirements, DoS-specific contracting requirements, country-specific constraints, and exemplar requests for proposals.\n",
       "2. **Data Preprocessing**: Clean, preprocess, and augment the sourced data using NVIDIA's Omniverse Replicator, which can generate high-quality, synthetic data.\n",
       "3. **Model Training**: Train the AI algorithms using the preprocessed data and fine-tune them with NVIDIA's TAO Toolkit, while leveraging the company's generative AI expertise.\n",
       "4. **Content, Style, and Structure Training**: Train the algorithm to generate responses and documents in the required content, style, and structure, with a focus on compliance with regulations and requirements.\n",
       "5. **Continuous Monitoring and Refining**: Regularly monitor and refine the AI algorithms to ensure they remain up-to-date with changing regulations and requirements, and maintain high accuracy and quality.\n",
       "\n",
       "**Benefits:**\n",
       "\n",
       "1. **Compliance**: Trained AI algorithms will ensure compliance with various regulations and requirements, reducing the risk of non-compliance.\n",
       "2. **Accuracy**: AI algorithms trained with NVIDIA technologies will generate accurate and compelling responses and documents.\n",
       "3. **Efficiency**: Leveraging NVIDIA Omniverse Replicator and TAO Toolkit will reduce the need for manual data curation and labeling, making the training process more efficient.\n",
       "4. **Scalability**: The trained AI algorithms will enable seamless integration with existing procurement systems, allowing for broader adoption and scalability.\n",
       "\n",
       "By leveraging NVIDIA technologies and WWT's generative AI expertise, WWT can develop and deploy accurate, compliant, and scalable AI-powered solutions that drive business success."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. **Integrate AI with Existing Procurement Systems**\n",
      "   Ensure AI algorithms can access data in relevant systems (e.g., Integrated Logistics Management System (ILMS)/Ariba, System for Award Management (SAM.gov), etc.) and that the created tool outputs can integrate seamlessly back into those systems.\n",
      "[Document(id='8e9400aa-61d7-477b-a969-190fd5e26571', metadata={'source': '../data/rfi-docs/Requirement+Writing+AI-RFI+v4+Final-07192023.pdf', 'page': 4, 'page_label': '5'}, page_content='07/19/2023 \\n5  \\n \\nTrain the AI Algorithms \\nIdentify and ascertain ownership of any datasets  to train the model to be able to draw from existing \\nFederal Acquisition Regulations, cybersecurity requirements, DoS-specific contracting requirements, \\ncountry-specific constraints, exemplar requests for proposals, as well as other data, when generating \\nstylistically and structurally appropriate responses and draft documents. Further, train the algorithm \\nfor content, style, structure, and compliance. \\n \\nIntegrate AI with Existing Procurement Systems   \\nEnsure AI algorithms can access the data existing in relevant systems (e.g., ILMS/ARIBA, and Sam.gov, \\netc.) and that the created tool outputs can integrate seamlessly back into those systems. \\n \\nMonitor and Refine'), Document(id='78c82e38-e6f5-4687-9fb1-86dedba7b667', metadata={'source': '../data/partner-docs/How Atom Ai and RFP Assistant have Evolved and What it Means for the Future - WWT.pdf', 'page': 1, 'page_label': '2'}, page_content=\"In a prior WWT Research Note, How WWT is Harnessing Generative AI to Drive\\nInternal Business Value, we gave readers a glimpse into our company's internal\\napproach to AI adoption. We outlined our methodology for assessing and validating\\ngenerative AI (GenAI) use cases and shared how we were beginning to apply that\\nframework to several internal use cases, including the ATC Assistant and the RFP\\nAssistant. \\nThis Research Note shares some updates from our AI journey thus far, with a focus on\\nhow these two intelligent digital assistant applications have evolved since they were\\nfirst conceived. Both applications leverage WWT's internal data sources to create\\ncustom solutions for different business needs.\\nAs a quick refresher, we designed the RFP Assistant to streamline the RFP intake\"), Document(id='d56fb642-0da1-4192-88c2-f1d0ef7a02d4', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_topics_ai_.txt'}, page_content='NVIDIA’s AI solutions workflow for enterprises The Most Advanced AI, Ready for Enterprise Transform any enterprise into an AI organization with full-stack innovation across accelerated infrastructure, enterprise-grade software, and AI models. By accelerating the entire AI workflow, projects reach production faster, with higher accuracy, efficiency, and infrastructure performance at a lower overall cost for various solutions and applications. Learn About NVIDIA’s AI Solutions References Visible links: 1. https://developer.nvidia.com/topics/ai 2. https://developer.nvidia.com/topics/ai 3. https://developer.nvidia.cn/topics/ai 4. https://developer.nvidia.com/topics/ 5. https://build.nvidia.com/explore/discover 6.'), Document(id='e6c52a86-a83d-41d5-ba3c-8d71b9b76051', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_nvidia-omniverse-platform_ace.txt'}, page_content='via NVIDIA NIM, which offers an easy- to-use microservice for high-performance deployment of AI models. NIM are available to deploy and operate in the cloud, or deploy models and run locally on NVIDIA RTX AI PCs. Digital Human Reference Workflows Developers can integrate individual—or all—ACE technologies directly into their products, tools, services, or games and experiences for domain-specific AI workflows such as NPCs and customer service assistants. Enhance Customer Interactions With Digital Human Technologies Customer Service NVIDIA Tokkio is a reference digital assistant workflow built with ACE, bringing AI-powered customer service capabilities to healthcare, IT, retail, and more. It brings assistants to life using state-of-the-art real-time language, speech, and animation')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Based on the provided context, World Wide Technologies (WWT) might leverage their generative AI expertise enhanced with NVIDIA technologies to address the section on **Integrate AI with Existing Procurement Systems** as follows:\n",
       "\n",
       "**Enabling Seamless Integration with Procurement Systems**\n",
       "\n",
       "To integrate their AI algorithms with existing procurement systems, WWT could leverage NVIDIA's AI solutions, specifically the **NVIDIA NIM** (NVIDIA Microservices) platform. NIM provides a high-performance deployment of AI models in the cloud or on-premises, enabling the efficient integration of AI algorithms with existing systems.\n",
       "\n",
       "**Key Strategies:**\n",
       "\n",
       "1.  **Data Access**: WWT could utilize NVIDIA NIM's ability to access and integrate with various data sources, including ILMS/Ariba and SAM.gov, to gather relevant data on procurement processes, contracts, and other procurement-related information.\n",
       "2.  **AI-Driven Insights**: By combining NVIDIA's AI capabilities with WWT's generative AI expertise, the company could develop AI-driven insights and analysis to enhance procurement decision-making and workflows.\n",
       "3.  **Seamless Integration**: Using NVIDIA's microservices architecture, WWT could ensure that the created tool outputs (e.g., RFP documents, contracts, or procurement reports) integrate seamlessly with existing procurement systems and applications.\n",
       "\n",
       "**Benefits:**\n",
       "\n",
       "1.  **Streamlined Procurement**: By integrating AI-driven tools with existing procurement systems, WWT could streamline procurement processes, reducing manual errors and improving overall efficiency.\n",
       "2.  **Improved Decision-Making**: NVIDIA's AI capabilities and WWT's generative AI expertise could enable data-driven insights and analysis, ultimately enhancing procurement decision-making and reducing the risk of costly errors.\n",
       "3.  **Increased Productivity**: By automating repetitive tasks and providing AI-driven support, WWT's integrated AI solution could increase the productivity of procurement teams, enabling them to focus on high-value tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. **Monitor and Refine**\n",
      "   Ensure the solution provides accurate and relevant insights by monitoring output and adjusting algorithms as necessary.\n",
      "[Document(id='e9b777b1-4a52-4284-89e0-f039dbc9029a', metadata={'source': '../data/partner-docs/Part 1_ Meet RFP Assistant — the AI Improving Proposal Response Speed and Efficiency - WWT.pdf', 'page': 8, 'page_label': '9'}, page_content='https://www.wwt.com/wwt-research/part-1-meet-rfp-assistant-the-ai-improving-proposal-response-speed-and-eﬃciency 9/21'), Document(id='15dda612-b7d3-4195-b824-ecb2f6ecf840', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_drive_drive-sim.txt'}, page_content='5. https://www.nvidia.com/en-us/omniverse/cloud/#next-steps 6. https://developer.nvidia.com/drive/simulation'), Document(id='0d3e11c0-4d6c-4ce7-a1d2-45de23daf824', metadata={'source': '../data/partner-docs/Part 2_ Inside RFP Assistant — Summarizer and Qualification Workflow - WWT.pdf', 'page': 0, 'page_label': '1'}, page_content='https://www.wwt.com/wwt-research/part-2-inside-rfp-assistant-summarizer-and-qualiﬁcation-workﬂow 1/13'), Document(id='dc527b84-3310-4c6e-960b-faf09c1b2f33', metadata={'source': '../data/nvidia-text-archive/developer.nvidia.com_rtx_ray-tracing_optix.txt'}, page_content='74. https://developer.nvidia.com/designworks/optix/download 75. https://developer.nvidia.com/rtx/ray-tracing/optix')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To address the **Monitor and Refine** section, World Wide Technologies (WWT) can leverage their generative AI expertise enhanced with NVIDIA technologies as follows:\n",
       "\n",
       "**Enhancing Model Performance with NVIDIA**\n",
       "\n",
       "WWT can utilize the power of NVIDIA's AI computing platforms, such as NVIDIA DGXs or NVIDIA V100, to improve the performance and accuracy of their AI models. By leveraging NVIDIA's high-performance computing capabilities, WWT can:\n",
       "\n",
       "1. **Train more complex models**: NVIDIA's V100 and other high-end GPUs enable WWT to train larger, more complex models that can provide more accurate insights.\n",
       "2. **Increase model speed**: NVIDIA's AI computing platforms can significantly accelerate the processing of AI workloads, enabling WWT to refine their models more efficiently.\n",
       "3. **Scale model deployment**: WWT can utilize NVIDIA's cloud services, such as NVIDIA Omniverse Cloud, to deploy their models at scale, ensuring seamless integration with their client's systems.\n",
       "\n",
       "**Monitoring Model Performance with NVIDIA's RTX and Optix**\n",
       "\n",
       "To monitor and refine the output of their AI models, WWT can leverage NVIDIA's RTX and Optix technologies. For instance:\n",
       "\n",
       "1. **Visualizing data with NVIDIA RTX**: NVIDIA RTX enables the creation of photorealistic visuals, allowing WWT to inspect the output of their models and refine their performance.\n",
       "2. **Using Optix for photorealistic rendering**: Optix, a toolkit for high-performance, photorealistic rendering, allows WWT to analyze and optimize the visual output of their models.\n",
       "3. **Enhancing model interpretability**: By visualizing the output of their models in a more human-understandable format, WWT can refine their AI models to provide more accurate and relevant insights.\n",
       "\n",
       "**Refining Models with NVIDIA's Drive and Simulation**\n",
       "\n",
       "To refine their AI models and ensure they provide accurate and relevant insights, WWT can leverage NVIDIA's Drive and Simulation technologies:\n",
       "\n",
       "1. **Simulating real-world scenarios**: WWT can simulate real-world scenarios within NVIDIA's simulation platforms, enabling them to refine their AI models and test them in various environments.\n",
       "2. **Data-driven model refinement**: NVIDIA's Drive and Simulation technologies provide the necessary infrastructure for WWT to refine their AI models using real-world data from simulated scenarios.\n",
       "\n",
       "By integrating NVIDIA's AI computing platforms, visualization, and simulation technologies into their AI expertise, WWT can effectively monitor and refine their generative AI solutions, ensuring they provide accurate and relevant insights for their clients."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "responses = []\n",
    "for section in task.content.split('\\n\\n'):\n",
    "    if section in [' ', '']:\n",
    "        continue\n",
    "    print(section)\n",
    "    query = f'Please highlight how World Wide Technologies (WWT) might leverage their generative AI expertise enhanced with NVIDIA technologies to address the following section: {section}'\n",
    "    response = rag_query(query, llm, vectorstore)\n",
    "    responses.append(response)\n",
    "    display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
